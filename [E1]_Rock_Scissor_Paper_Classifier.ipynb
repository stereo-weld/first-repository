{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 프로젝트: 가위바위보 분류기 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터를 준비하자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) 필요한 모듈 import하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os, glob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) 데이터 준비"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2-1) 데이터 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) 구글의 teachable machine 사이트(https://teachablemachine.withgoogle.com/)에 접속한다.  \n",
    "(2) 노트북 전면 카메라를 활용하여 가위, 바위, 보 이미지 각 100장을 만든다.  \n",
    "(3) 나(100x3=300 jpg images)와 18명(16x100x3=4800 jpg images) 그리고 1팀(840x3=2520 png images)의 이미지 압축파일을 다운로드 했다.  \n",
    "(4) rock_scissor_paper 라는 폴더 아래에 scissor, rock, paper 폴더를 만들어서 이미지를 저장한다.  \n",
    "(5) 압축파일을 unzip으로 해제하고, 각 폴더에서 이미지 파일 이외의 것은 제거하고 , png 이미지의 경우에 jpg 이미지로 확장자를 변경한다.  \n",
    "(6) 각 사람별로 scissor, rock, paper 폴더에 있는 동일한 이름의 파일들을 하나의 scissor, rock, paper 폴더에 새로운 이름으로 저장한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2-2) 데이터 불러오기 + Resize 하기\n",
    "\n",
    "(5) 숫자 손글씨의 경우 이미지 크기가 28x28 이었기 때문에, 우리의 가위, 바위, 보 이미지도 28x28로 만들어야 합니다. 이를 위해서는 PIL 라이브러리를 사용해볼 거예요. 그러려면 먼저 라이브러리를 불러와야 겠죠?\n",
    "혹시 PIL 라이브러리가 없는 경우 필요한 패키지를 설치해 주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이미지 디렉토리 경로:  /home/aiffel/aiffel/rock_scissor_paper/scissor\n",
      "가위 이미지 개수: 2640\n",
      "가위 이미지 resize 완료!\n"
     ]
    }
   ],
   "source": [
    "# 가위 이미지가 저장된 디렉토리 아래의 모든 jpg 파일을 읽어들여서\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper_train/scissor\"\n",
    "print(\"이미지 디렉토리 경로: \", image_dir_path)\n",
    "# glob() 함수는 인자로 받은 패턴과 이름이 일치하는 모든 파일과 디렉터리의 리스트를 반환합니다. \n",
    "images=glob.glob(image_dir_path + \"/*.jpg\")  \n",
    "\n",
    "# 파일마다 모두 28x28 사이즈로 바꾸어 저장합니다.\n",
    "target_size=(28,28)\n",
    "for img in images:\n",
    "    old_img=Image.open(img)\n",
    "    old_img=old_img.convert(\"RGB\")\n",
    "    new_img=old_img.resize(target_size, Image.ANTIALIAS)\n",
    "    new_img.save(img, format=\"JPEG\", quality=100)\n",
    "\n",
    "print(\"가위 이미지 개수:\", len(os.listdir(image_dir_path)))\n",
    "print(\"가위 이미지 resize 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이미지 디렉토리 경로: /home/aiffel/aiffel/rock_scissor_paper/rock\n",
      "바위 이미지 개수: 2640\n",
      "바위 이미지 resize 완료!\n"
     ]
    }
   ],
   "source": [
    "# 바위 이미지가 저장된 디렉토리 아래의 모든 jpg 파일을 읽어들여서\n",
    "image_dir_path = os.getenv(\"HOME\")+\"/aiffel/rock_scissor_paper_train/rock\"\n",
    "print(\"이미지 디렉토리 경로:\", image_dir_path)\n",
    "\n",
    "images = glob.glob(image_dir_path + \"/*.jpg\")\n",
    "\n",
    "# 파일마다 모두 28x28 사이즈로 바꾸어 저장합니다.\n",
    "target_size = (28, 28)\n",
    "for img in images:\n",
    "  old_img = Image.open(img)\n",
    "  old_img=old_img.convert(\"RGB\")\n",
    "  new_img = old_img.resize(target_size, Image.ANTIALIAS)\n",
    "  new_img.save(img, \"JPEG\")\n",
    "  \n",
    "print(\"바위 이미지 개수:\", len(os.listdir(image_dir_path)))\n",
    "print(\"바위 이미지 resize 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이미지 디렉토리 경로: /home/aiffel/aiffel/rock_scissor_paper/paper\n",
      "보 이미지 개수: 2640\n",
      "보 이미지 resize 완료!\n"
     ]
    }
   ],
   "source": [
    "# 보 이미지가 저장된 디렉토리 아래의 모든 jpg 파일을 읽어들여서\n",
    "image_dir_path = os.getenv(\"HOME\")+\"/aiffel/rock_scissor_paper_train/paper\"\n",
    "print(\"이미지 디렉토리 경로:\", image_dir_path)\n",
    "\n",
    "images = glob.glob(image_dir_path + \"/*.jpg\")\n",
    "\n",
    "\n",
    "# 파일마다 모두 28x28 사이즈로 바꾸어 저장합니다.\n",
    "target_size = (28, 28)\n",
    "for img in images:\n",
    "  old_img = Image.open(img)\n",
    "  old_img=old_img.convert(\"RGB\")\n",
    "  new_img = old_img.resize(target_size, Image.ANTIALIAS)\n",
    "  new_img.save(img, \"JPEG\")\n",
    "\n",
    "print(\"보 이미지 개수:\", len(os.listdir(image_dir_path)))\n",
    "print(\"보 이미지 resize 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습데이터(x_train)의 이미지 개수는 0 입니다.\n",
      "학습데이터(x_train)의 이미지 개수는 0 입니다.\n",
      "x_train shape: (7920, 28, 28, 3)\n",
      "y_train shape: (7920,)\n",
      "x_test shape: (300, 28, 28, 3)\n",
      "y_test shape: (300,)\n"
     ]
    }
   ],
   "source": [
    "def load_data(img_path, num_imgs):\n",
    "    # 가위 : 0, 바위 : 1, 보 : 2\n",
    "    number_of_data = num_imgs   # 가위바위보 이미지 개수 총합에 주의하세요.\n",
    "    img_size = 28\n",
    "    color = 3\n",
    "    \n",
    "    #이미지 데이터와 라벨(가위 : 0, 바위 : 1, 보 : 2) 데이터를 담을 행렬(matrix) 영역을 생성합니다.\n",
    "    imgs = np.zeros(number_of_data*img_size*img_size*color,dtype=np.int32).reshape(number_of_data, img_size, img_size, color)\n",
    "    labels = np.zeros(number_of_data, dtype=np.int32)\n",
    "\n",
    "    idx=0\n",
    "    for file in glob.iglob(img_path+'/scissor/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=0   # 가위 : 0\n",
    "        idx=idx+1\n",
    "\n",
    "    for file in glob.iglob(img_path+'/rock/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=1   # 바위 : 1\n",
    "        idx=idx+1       \n",
    "    \n",
    "    for file in glob.iglob(img_path+'/paper/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=2   # 보 : 2\n",
    "        idx=idx+1\n",
    "        \n",
    "    print(\"학습데이터(x_train)의 이미지 개수는\",idx,\"입니다.\")\n",
    "    return imgs, labels\n",
    "\n",
    "train_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper_train\"\n",
    "test_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper_test\"\n",
    "\n",
    "(x_train, y_train) = load_data(image_dir_path, 2640*3)\n",
    "(x_test, y_test) = load_data(image_dir_path, 100*3)\n",
    "x_train_norm = x_train/255.0   # 입력은 0~1 사이의 값으로 정규화\n",
    "x_test_norm = x_test/255.0   # 입력은 0~1 사이의 값으로 정규화\n",
    "\n",
    "\n",
    "print(\"x_train shape: {}\".format(x_train.shape))\n",
    "print(\"y_train shape: {}\".format(y_train.shape))\n",
    "print(\"x_test shape: {}\".format(x_test.shape))\n",
    "print(\"y_test shape: {}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) 데이터 이해하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "라벨:  0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(x_train[0])\n",
    "print('라벨: ', y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4) train, test 데이터 분리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train data : 다른 사람들의 데이터 2640 x 3 = 7920 images\n",
    "#### test data : 나의 데이터 100 x 3 = 300 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X_train, X_valid, y_train, y_valid = train_test_split(x_train, \n",
    "#                                                     y_train, \n",
    "#                                                     valid_size=0.2, \n",
    "#                                                     random_state=7)\n",
    "\n",
    "# print('X_train 개수: ', len(X_train), ', X_valid 개수: ', len(X_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.cross_validation import train_test_split \n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1) \n",
    "# X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_size = 0.6\n",
    "# validate_size = 0.2\n",
    "# train, validate, test = np.split(my_data.sample(frac=1), [int(train_size * len(my_data)), int((validate_size + train_size) * len(my_data))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 딥러닝 네트워크 (모델) 설계하기\n",
    "자 이제 데이터의 준비가 끝났습니다. 이제 여러분들이 가위바위보를 인식하는 딥러닝 네트워크를 설계해 볼까요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                51232     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 70,723\n",
      "Trainable params: 70,723\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "n_channel_1 = 32\n",
    "n_channel_2 = 64\n",
    "n_dense = 32\n",
    "n_train_epoch = 10\n",
    "\n",
    "model=keras.models.Sequential()\n",
    "model.add(keras.layers.Conv2D(n_channel_1, (3,3), activation='relu', input_shape=(28,28,3)))\n",
    "model.add(keras.layers.MaxPool2D(2,2))\n",
    "model.add(keras.layers.Conv2D(n_channel_2, (3,3), activation='relu'))\n",
    "model.add(keras.layers.MaxPooling2D((2,2)))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(n_dense, activation='relu'))\n",
    "model.add(keras.layers.Dense(3, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (5) 딥러닝 네트워크 (모델) 학습시키기\n",
    "잘 설계가 되었다면, 이제 학습을 시켜봅시다. 아마도 여러분들의 데이터는 거의 비슷비슷할 것이기 때문에 accuracy가 꽤 높게 나올 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 1.0201 - accuracy: 1.0000\n",
      "Epoch 2/10\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 0.7482 - accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 0.5495 - accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 0.4085 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 0.3092 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 0.2387 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 0.1877 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 0.1502 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 0.1219 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 0.1002 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f7e4c2e8810>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 학습 방식에 대한 환경설정\n",
    "model.compile(optimizer='adam',\n",
    "             loss='sparse_categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "# 모델 훈련\n",
    "model.fit(x_train_norm, y_train, epochs=n_train_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (6) 얼마나 잘 만들었는지 확인하기(테스트)\n",
    "여러분들은 300장의 가위바위보 이미지를 만들어 모두 학습에 사용했습니다. 그러므로 테스트 데이터가 없죠. 옆 친구의 이미지 데이터 300장을 받아오세요. 그리고 그것을 테스트 데이터로 하여 test accuracy를 측정해보세요.\n",
    "\n",
    "우선 테스트용 데이터인 x_test, y_test를 만들어 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습데이터(x_train)의 이미지 개수는 300 입니다.\n",
      "x_test shape: (300, 28, 28, 3)\n",
      "y_test shape: (300,)\n"
     ]
    }
   ],
   "source": [
    "# x_test, y_test를 만드는 방법은 x_train, y_train을 만드는 방법과 아주 유사합니다.\n",
    "def load_data(img_path, num_imgs):\n",
    "    # 가위 : 0, 바위 : 1, 보 : 2\n",
    "    number_of_data = num_imgs   # 가위바위보 이미지 개수 총합에 주의하세요.\n",
    "    img_size = 28\n",
    "    color = 3\n",
    "    #이미지 데이터와 라벨(가위 : 0, 바위 : 1, 보 : 2) 데이터를 담을 행렬(matrix) 영역을 생성합니다.\n",
    "    imgs=np.zeros(number_of_data*img_size*img_size*color,dtype=np.int32).reshape(number_of_data,img_size,img_size,color)\n",
    "    labels=np.zeros(number_of_data,dtype=np.int32)\n",
    "\n",
    "    idx=0\n",
    "    for file in glob.iglob(img_path+'/scissor/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=0   # 가위 : 0\n",
    "        idx=idx+1\n",
    "\n",
    "    for file in glob.iglob(img_path+'/rock/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=1   # 바위 : 1\n",
    "        idx=idx+1       \n",
    "    \n",
    "    for file in glob.iglob(img_path+'/paper/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=2   # 보 : 2\n",
    "        idx=idx+1\n",
    "        \n",
    "    print(\"학습데이터(x_train)의 이미지 개수는\",idx,\"입니다.\")\n",
    "    return imgs, labels\n",
    "\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper_test\"\n",
    "(x_test, y_test) = load_data(image_dir_path, 100*3)\n",
    "x_test_norm = x_test/255.0   # 입력은 0~1 사이의 값으로 정규화\n",
    "\n",
    "print(\"x_test shape: {}\".format(x_test.shape))\n",
    "print(\"y_test shape: {}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 - 0s - loss: 2.0839 - accuracy: 0.3333\n",
      "test_loss: 2.083932399749756 \n",
      "test_accuracy: 0.3333333432674408\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(x_test_norm, y_test, verbose=2)\n",
    "\n",
    "print(\"test_loss: {} \".format(test_loss))\n",
    "print(\"test_accuracy: {}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 더 좋은 네트워크 만들어보기\n",
    "시험용 데이터(x_test)에 대한 인식률(test accuracy)이 train accuracy보다 많이 낮게 나오지는 않았나요?\n",
    "만약 그렇다면 그 이유는 무엇일까요? MNIST 손글씨 데이터 때처럼 test accuracy가 train accuracy에 근접하도록 개선 방법을 찾아 봅시다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "에러메시지 : OSError: cannot write mode RGBA as JPEG\n",
    "오류원인 : jpg 파일은 투명도를 표현할 수 없는 파일 포멧인데 여기에 alpha값을 저장하려고 할 경우 발생되어집니다.\n",
    "해결방법 : im = im.convert(\"RGB\") im.save('python.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 이미지를 섞지 않은 경우  \n",
    " -- 10/10 - 0s - loss: 2.0839 - accuracy: 0.3333"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def rand_shuffle(x_data, y_data):\n",
    "    tmp = [[x,y] for x, y in zip(x_data, y_data)]\n",
    "\n",
    "    random.shuffle(tmp)\n",
    "\n",
    "    x_data_sf = [n[0] for n in tmp]\n",
    "    y_data_sf = [n[1] for n in tmp]\n",
    "    \n",
    "    return x_data_sf, y_data_sf\n",
    "\n",
    "x_train_shuffle, y_train_shuffle = rand_shuffle(x_train_norm, y_train)\n",
    "x_test_shuffle, y_test_shuffle = rand_shuffle(x_test_norm, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(x_train_norm, y_train, test_size=0.2, random_state=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to find data adapter that can handle input: (<class 'list'> containing values of types {\"<class 'numpy.ndarray'>\"}), (<class 'list'> containing values of types {\"<class 'numpy.int32'>\"})",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-c2388ed4cc99>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# 모델 훈련\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_shuffle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_shuffle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1062\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1063\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1064\u001b[0;31m           steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[1;32m   1065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1066\u001b[0m       \u001b[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001b[0m\n\u001b[1;32m   1097\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_steps_per_execution_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msteps_per_execution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1099\u001b[0;31m     \u001b[0madapter_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_data_adapter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1100\u001b[0m     self._adapter = adapter_cls(\n\u001b[1;32m   1101\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mselect_data_adapter\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    962\u001b[0m         \u001b[0;34m\"Failed to find data adapter that can handle \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m         \"input: {}, {}\".format(\n\u001b[0;32m--> 964\u001b[0;31m             _type_name(x), _type_name(y)))\n\u001b[0m\u001b[1;32m    965\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madapter_cls\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m     raise RuntimeError(\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to find data adapter that can handle input: (<class 'list'> containing values of types {\"<class 'numpy.ndarray'>\"}), (<class 'list'> containing values of types {\"<class 'numpy.int32'>\"})"
     ]
    }
   ],
   "source": [
    "# 모델 학습 방식에 대한 환경설정\n",
    "model.compile(optimizer='adam',\n",
    "             loss='sparse_categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "# 모델 훈련\n",
    "model.fit(x_train_shuffle, y_train_shuffle, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 - 0s - loss: 0.0092 - accuracy: 1.0000\n",
      "valid_loss: 0.009177171625196934 \n",
      "valid_accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "valid_loss, valid_accuracy = model.evaluate(X_valid, y_valid, verbose=2)\n",
    "\n",
    "print(\"valid_loss: {} \".format(valid_loss))\n",
    "print(\"valid_accuracy: {}\".format(valid_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 - 0s - loss: 21.4697 - accuracy: 0.3333\n",
      "test_loss: 21.469654083251953 \n",
      "test_accuracy: 0.3333333432674408\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(x_test, y_test, verbose=2)\n",
    "\n",
    "print(\"test_loss: {} \".format(test_loss))\n",
    "print(\"test_accuracy: {}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.predict() 결과 :  [0.98948294 0.00501556 0.00550145]\n",
      "model이 추론한 가장 가능성이 높은 결과 :  0\n",
      "실제 데이터의 라벨 :  0\n"
     ]
    }
   ],
   "source": [
    "predicted_result = model.predict(x_test_norm)  # model이 추론한 확률값. \n",
    "predicted_labels = np.argmax(predicted_result, axis=1)\n",
    "\n",
    "idx=0  #1번째 x_test를 살펴보자. \n",
    "print('model.predict() 결과 : ', predicted_result[idx])\n",
    "print('model이 추론한 가장 가능성이 높은 결과 : ', predicted_labels[idx])\n",
    "print('실제 데이터의 라벨 : ', y_test[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
